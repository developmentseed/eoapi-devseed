#!/usr/bin/env python

import argparse
import io
import json

import pystac_client
from pystac.media_type import MediaType
from pystac.extensions.item_assets import AssetDefinition, ItemAssetsExtension
from pypgstac.db import PgstacDB
from pypgstac.load import Loader, Methods


def bbox_type(value):
    return [float(coord) for coord in value.split(",")]


RENDER_PARAMS = {
    "io-10m-annual-lulc": {
        "land_cover": {
            "assets": ["supercell"],
            "colormap": {
                0: (0, 0, 0, 1),
                1: (65, 155, 223, 1),
                2: (57, 125, 73, 1),
                4: (122, 135, 198, 1),
                5: (228, 150, 53, 1),
                7: (196, 40, 27, 1),
                8: (165, 155, 143, 1),
                9: (168, 235, 255, 1),
                10: (97, 97, 97, 1),
                11: (227, 226, 195, 1),
            },
            "resampling": "nearest",
            "minzoom": 4,
            "maxzoom": 9,
        },
    }
}

ITEM_ASSETS = {
    "io-10m-annual-lulc": {
        "supercell": AssetDefinition(
            {
                "type": MediaType.COG,
                "roles": ["data"],
                "title": "Annual Land Use and Land Cover",
                "description": (
                    "Time series of annual global maps of land use and land cover (LULC). "
                    "It currently has data from 2017-2023. The maps are derived from ESA "
                    "Sentinel-2 imagery at 10m resolution. Each map is a composite of LULC "
                    "predictions for 9 classes throughout the year in order to generate a "
                    "representative snapshot of each year. This dataset was generated by "
                    "Impact Observatory, who used billions of human-labeled pixels (curated "
                    "by the National Geographic Society) to train a deep learning model for "
                    "land classification. The global map was produced by applying this model "
                    "to the Sentinel-2 annual scene collections on the Planetary Computer. "
                    "Each of the maps has an assessed average accuracy of over 75%. All years "
                    "are available under a Creative Commons BY-4.0."
                ),
            }
        )
    }
}


def main():
    parser = argparse.ArgumentParser(
        description=(
            "Load existing STAC collection and items into a pgstac database."
            "Set the PG* environment variables to configure pgstac to point at "
            "your database!"
        )
    )

    parser.add_argument(
        "stac_api_url",
        type=str,
        help="STAC API URL",
    )

    parser.add_argument(
        "collection_id",
        type=str,
        help="collection ID",
    )
    parser.add_argument(
        "--bbox",
        type=bbox_type,
        help="Bounding box coordinates as comma-separated values (e.g., minx,miny,maxx,maxy)",
        default=None,
        required=False,
    )

    args = parser.parse_args()

    # fire up pgstac loader
    db = PgstacDB()
    loader = Loader(db=db)

    # connect to the Impact Observatory STAC API
    client = pystac_client.Client.open(args.stac_api_url)

    collection = client.get_collection(args.collection_id)
    collection.clear_links()

    # item assets extension
    item_assets_ext = ItemAssetsExtension.ext(collection, add_if_missing=True)

    if item_assets := ITEM_ASSETS.get(args.collection_id):
        item_assets_ext.item_assets = item_assets

    if render_params := RENDER_PARAMS.get(args.collection_id):
        collection.extra_fields["renders"] = render_params

    if not collection:
        raise ValueError(
            f"No collection with id {args.collection_id} found in {args.stac_api_url}"
        )

    loader.load_collections(
        io.BytesIO(json.dumps(collection.to_dict()).encode("utf-8")),
        insert_mode=Methods.upsert,
    )

    print("processing items")

    search = client.search(collections=[args.collection_id], bbox=args.bbox)

    item_collection = []
    for item in search.item_collection():
        item.clear_links()
        item_collection.append(item)

    loader.load_items(
        (item.to_dict() for item in item_collection),
        insert_mode=Methods.upsert,
    )


if __name__ == "__main__":
    main()
